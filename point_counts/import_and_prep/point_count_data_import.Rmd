---
title: "Site C Point Count Data: Import, QA and Prep"
output:
  html_notebook:
    toc: TRUE
    toc_float: TRUE
---

This script is for import, tidy and quality assurance checks of the survey data.

To keep referential integrity, it seems best to join the three tables (stations, surveys and obs).

That way the visits can be properly renumbered (although that really only matter for the visit analysis)

# Setup

```{r message=FALSE, include=FALSE}
# Packages
library(tidyverse)
library(readxl)
library(lubridate)
library(skimr)
library(here)
library(kableExtra)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
options(scipen = 999)
```

# Read and tidy data

The data imported below is a copy of the master data file that is located in the project Sharepoint site. Make sure it is up to date.

Set some nicer names for variables regularly used. Get rid of variables that will not be used in any analysis, just for cleaner tables.

```{r message=FALSE, warning=FALSE}
fname <- "Songto2021data.xlsx"

stations <- read_excel(here("point_counts", "data_raw", paste0(fname)), "Stations") %>% 
  dplyr::rename(StationID = 'Sample Station Label',
         Easting = `Easting Sample Station`,
         Northing = `Northing Sample Station`) %>%
  dplyr::mutate(Year = str_sub(`Survey Name`,-4)) %>% 
  dplyr::select(StationID, Year, everything(), 
         -c(`Sample Station Photos`, `Sample Station Comments`,
            `UTM Zone Sample Station`, `Survey Name`, Station))

surveys <- read_excel(here("point_counts", "data_raw", paste0(fname)), 
                      "Surveys") %>% 
  dplyr::rename(StationID = 'Sample Station Label') %>%
  dplyr::select(StationID, Visit, Date, Time, `Survey Duration`, `Inventory Type`) %>% 
  dplyr::mutate(Date = as_date(Date), 
         Time = as_datetime(Time)) 
# Update the date in "Time" with correct date from `Date` 
date(surveys$Time) <- date(surveys$Date)

# Most analysis of the songbird data must account for temporal survey-level effects
# These variables can be created now and to be used later.
# Year, TSSR, YDAY, DSLS

obs <- read_excel(here("point_counts", "data_raw", paste0(fname)), 
                  "Observations",
                  col_types = c("text", "numeric", "text", "numeric", "text", 
                                "numeric", "numeric", "numeric", "numeric", 
                                "numeric", "text", "text", "text", "text", 
                                "numeric", "numeric", "text", "guess", "guess",
                                "guess", "guess"),
                  na = "NA") %>% 
  dplyr::rename(StationID = 'Sample Station Label', SpCode = Species) %>% 
  dplyr::select(StationID, Visit, SpCode, `Count 5 min`, `Count 0-3 min`, `Count 3-5 min`, 
         `Count 5-10 min`, Count, `Distance Category`, Flyovers) %>% 
  dplyr::filter(str_detect(SpCode, "B-U", negate = TRUE)) # Clear out the unknowns
```

# Bring in the BC Bird list from BC Species and Ecosystem Explorer

```{r}
BCbirds <- read_excel(here("point_counts", "data_reference", 
                           "summaryExport_bcsee_2021-10-21.xlsx")) %>% 
  dplyr::select(`English Name`, `Scientific Name`, `Species Code`, Order, 
         `BC List`, COSEWIC, `SARA Status`, `Breeding Bird`) %>% 
  rename(SpCode = "Species Code") %>%
  mutate(IsSongbird = ifelse(Order %in% c("Columbiformes", "Caprimulgiformes", 
                                          "Piciformes", "Passeriformes"), 
                             TRUE, 
                             FALSE),
         sort = row_number()) %>% 
  filter(!is.na(SpCode))
```

# Check for errors

Check for duplicates in stations\$StationID.

```{r}
stations %>% 
  count(StationID) %>% 
  filter(n>1)
```

Check for duplicates in surveys\$StationID & Visit.

Note: There are some duplicates in 2011 data. Need to fix (2020-06-29).

```{r}
surveys %>% 
  count(StationID, Visit) %>% 
  filter(n>1)
```

Check to see if there is a match for StationID between stations and surveys. This can be done using a full join and then filter.

```{r}
# Join the two tables
StationID_match <- stations %>% 
  full_join(surveys, by = "StationID")
# Filter to check for StationID in `surveys` but not in `stations`.
StationID_match %>% filter(is.na(Year))
# Filter to check for StationID in `stations` but not in `surveys`.
StationID_match %>% filter(is.na(Visit))
```

Now check for match for StationID between 'surveys' and 'obs'. Note that stations and surveys must be clean before this is useful.

For the second "test", surveys that had no observations will also be listed.

```{r paged.print=TRUE}
StationID_match <- surveys %>% 
  full_join(obs, by = c("StationID", "Visit"))
# Filter to check for `StationID $ Visit` in `obs` but not in `surveys`.
StationID_match %>% filter(is.na(Date))
# Filter to check for `StationID $ Visit` in `surveys` but not in `obs`.
# NB: This might be because of no birds observations rather than data entry error.
StationID_match %>% filter(is.na(SpCode))
```

Validate species codes in obs table.

NONE is used to indicate a survey with no observations.

```{r}
obs %>% 
  left_join(BCbirds, by = "SpCode") %>% 
  filter(is.na(`English Name`))
```

Check missing values (nulls).

```{r}
is.null(stations)
is.null(surveys)
is.null(obs)
```

# Create summary tables to see if any values out of range

Map station coordinates for anything out of range.

```{r}
stations %>% 
  ggplot(aes(x = Easting, y = Northing, colour = Year)) + 
  geom_point()
```

Summary by number of visits.

```{r message=FALSE}
visit_sum <- surveys %>% 
  group_by(StationID, Year = year(Date)) %>% 
  summarize(Visits = n()) %>% 
  mutate(Visits = as_factor(Visits)) %>% 
  group_by(Year, Visits) %>% 
  summarize("Number of Locations" = n())

# Make a summary table
visit_sum %>% 
  pivot_wider(id_cols = "Visits", 
              names_from = "Year", 
              values_from = "Number of Locations") %>% 
  replace(is.na(.), 0) %>% 
  kable()

# Plot for presentation
visit_sum %>% 
  ggplot(aes(x = as.character(Year), y = `Number of Locations`,
             fill = Visits)) +
  geom_bar(stat = "identity") +
#  scale_fill_brewer(palette="reds") +
  xlab("Year")
```

```{r paged.print=TRUE}
# Survey dates and times.
table(year(surveys$Date))
table(month(surveys$Date))
table(hour(surveys$Time))

# Observations counts
table(obs$Count)
# Distances
table(obs$`Distance Category`)
```

# Create wide format

-   We have two types of point count surveys:

    -   5-min fixed radius

    -   10-min unlimited radius

There are various possible data sets that can be generated, depending on the analysis needed. However there are three basic datasets that can be subset if something different needed.

1.  All data:

    -   Both 5- and 10-min surveys and observations at all distances.

    -   Good for total richness.

    -   Can be used with QPAD since QPAD can accommodate the different durations and survey distances.

    -   Not good for average station richness or for rarefaction.

2.  Fixed 100m / 5-min surveys.

    -   Good for analysis over max number of years.

    -   Good for analyses that cannot accommodate varying survey distance and duration. For example rarefaction.

-   Filter for songbirds only.

-   Then reformat data to wide format for use in vegan: samples by species.

-   To make the wide matrix, it is necessary to join up with the survey data. This is because there were some surveys with no species detections and therefore do not appear in the observations table.

Make list of songbirds to filter data on:

```{r message=FALSE}
song_list <- BCbirds %>%
filter(IsSongbird == TRUE)
song_list <-song_list$SpCode

```

Create the filtered list of obs. Files written to 'data_processed' so that subsequent analyses can load the data without running the scripts here.

For all duration all distance data:

```{r message=FALSE}
song_wide_all <- obs %>% 
  left_join(surveys, by = c("StationID", "Visit")) %>% 
#  filter(`Distance Category` != ">100") %>%
  filter(SpCode %in% song_list) %>% 
  filter(is.na(Flyovers)) %>% 
  group_by(StationID, Visit, SpCode) %>% 
  summarise(count = sum(Count)) %>% 
  pivot_wider(names_from = "SpCode", values_from = count, values_fill = 0) %>% 
  select(sort(names(.))) %>% 
  select(StationID, Visit, everything()) %>% 
  full_join(surveys, by = c("StationID", "Visit")) %>% 
  left_join(stations, by = "StationID") %>%
  select(StationID, Visit, Year, Footprint, `Mitigation Property`, Valley,
         ValleyUpstPine, ValleyUpstDam, BBFM_ID, 
         Easting, Northing, Longitude, Latitude, BHC20, Date, Time,
         `Survey Duration`, `B-ALFL`:`B-YRWA`) %>% 
  replace(is.na(.), 0) %>% 
  arrange(StationID)

# write_csv(song_wide_all, here("point_counts", "data_processed", "song_wide_all.csv"))
```

For the 5-min, fixed 100m counts:

```{r message=FALSE}
song_wide_5min_100m <- obs %>% 
  left_join(surveys, by = c("StationID", "Visit")) %>% 
  rowwise() %>% 
  mutate(count5 = 
           case_when(year(Date) > 2016 ~ sum(c(`Count 0-3 min`, `Count 3-5 min`),
                                             na.rm = TRUE),
                     TRUE ~ `Count 5 min`)) %>% 
  filter(`Distance Category` != ">100") %>%
  filter(SpCode %in% song_list) %>% 
  filter(is.na(Flyovers)) %>% 
  group_by(StationID, Visit, SpCode) %>% 
  summarise(count = sum(count5)) %>% 
  pivot_wider(names_from = "SpCode", values_from = count, values_fill = 0) %>% 
  select(sort(names(.))) %>% 
  select(StationID, Visit, everything()) %>% 
  full_join(surveys, by = c("StationID", "Visit")) %>% 
  left_join(stations, by = "StationID") %>%
  select(StationID, Visit, Year, Footprint, `Mitigation Property`, Valley,
         ValleyUpstPine, ValleyUpstDam, BBFM_ID, 
         Easting, Northing, Longitude, Latitude, BHC20, Date, Time, 
         `Survey Duration`, `B-ALFL`:`B-YRWA`) %>% 
  replace(is.na(.), 0) %>% 
  arrange(StationID)

# write_csv(song_wide_5min_100m, here("point_counts", "data_processed", "song_wide_5min_100m.csv"))
    
```

5-minute unlimited.

```{r message=FALSE}
song_wide_5min_unlim <- obs %>% 
  left_join(surveys, by = c("StationID", "Visit")) %>% 
  rowwise() %>% 
  mutate(count5 = 
           case_when(year(Date) > 2016 ~ sum(c(`Count 0-3 min`, `Count 3-5 min`),
                                             na.rm = TRUE),
                     TRUE ~ `Count 5 min`)) %>% 
#  filter(`Distance Category` != ">100") %>%
  filter(SpCode %in% song_list) %>% 
  filter(is.na(Flyovers)) %>% 
  group_by(StationID, Visit, SpCode) %>% 
  summarise(count = sum(count5)) %>% 
  pivot_wider(names_from = "SpCode", values_from = count, values_fill = 0) %>% 
  select(sort(names(.))) %>% 
  select(StationID, Visit, everything()) %>% 
  full_join(surveys, by = c("StationID", "Visit")) %>% 
  left_join(stations, by = "StationID") %>%
  select(StationID, Visit, Year, Footprint, `Mitigation Property`, Valley,
         ValleyUpstPine, ValleyUpstDam, BBFM_ID, 
         Easting, Northing, Longitude, Latitude, BHC20, Date, Time, 
         `Survey Duration`, `B-ALFL`:`B-YRWA`) %>% 
  replace(is.na(.), 0) %>% 
  arrange(StationID)

# write_csv(song_wide_5min_unlim, here("point_counts", "data_processed", "song_wide_5min_unlim.csv"))
```

Clean up temp files.

```{r}
rm(StationID_match)
rm(visit_sum)
```

# 
